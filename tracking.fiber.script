#!/bin/bash
#SBATCH --time=24:0:0
#SBATCH --nodes=2
#SBATCH --cpus-per-task=8
#SBATCH --mem-per-cpu=32000
#SBATCH --job-name=tckgen
#SBATCH --partition=short
#SBATCH --mail-type=ALL
#SBATCH --mail-user=ai.me@northeastern.edu
#SBATCH --array=1-154%50

	module load singularity
	export SINGULARITY_BIND="/scratch/ai.me/data:/mnt,/shared/centos7"
	subject_name=`sed "${SLURM_ARRAY_TASK_ID}q;d" sublist_QC.txt`
	PP_DIR_NS=/scratch/ai.me/data/preprocessed_data/${subject_name} #path to pre-processed folder OUTSIDE singularity 
	PP_DIR=/mnt/preprocessed_data/${subject_name}
	

	mx=/shared/container_repository/MRtrix/mrtrix3_3.0.4.sif
	
	#a 10millions of streamlines will be generated with a threshold of FOD values = 0.06.
	singularity exec ${mx} tckgen -act ${PP_DIR}/5tt_hsvs2MEANB0.mif -backtrack -seed_gmwmi ${PP_DIR}/gmwmSeed_hsvs2MEANB0.mif -nthreads 8 -maxlength 250 -cutoff 0.06 -select 10000000 ${PP_DIR}/WM_FOD_norm.mif ${PP_DIR}/TK_10M.tck 
	# this is to generate a file with less streamlines so we can visualize it in mrview
	singularity exec ${mx} tckedit ${PP_DIR}/TK_10M.tck -number 200k ${PP_DIR}/smallerTK_200k.tck
